{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/56cd2750-6cbc-482b-9d27-210223b14d22\n",
    "\n",
    "### Question 1: Hierarchical Clustering\n",
    "\n",
    "#### Part (a)\n",
    "\n",
    "To find the first five joins in a hierarchical clustering model with complete linkage, follow these steps:\n",
    "\n",
    "1. **Initial Distances**:\n",
    "   ```\n",
    "   > dist(dX)\n",
    "     1    2    3    4    5    6    7    8    9\n",
    "   2  7.41\n",
    "   3  5.89 4.49\n",
    "   4  8.06 2.02 3.51\n",
    "   5  7.22 2.12 2.70 0.89\n",
    "   6  2.43 5.24 5.11 6.28 5.55\n",
    "   7  3.52 9.46 9.16 10.66 9.93 4.38\n",
    "   8  5.86 1.65 3.28 2.47 1.90 3.86 8.22\n",
    "   9  4.66 6.04 7.62 7.71 7.22 2.91 4.25 5.32\n",
    "   10 4.52 6.43 7.83 8.06 7.54 2.97 3.86 5.64 0.42\n",
    "   ```\n",
    "\n",
    "2. **Sorted Distances**:\n",
    "   ```\n",
    "   > sort(dX)\n",
    "   [1] 0.42 0.89 1.65 1.90 2.02 2.12 2.43 2.47 2.70 2.91\n",
    "   [11] 2.97 3.28 3.51 3.52 3.86 3.86 4.25 4.38 4.49 4.52\n",
    "   [21] 4.66 5.11 5.24 5.32 5.55 5.64 5.86 5.89 6.04 6.28\n",
    "   [31] 6.43 7.22 7.22 7.41 7.54 7.62 7.71 7.83 8.06 8.06\n",
    "   [41] 8.22 9.16 9.46 9.93 10.66\n",
    "   ```\n",
    "\n",
    "3. **First Five Joins**:\n",
    "   - Join 9 and 10 (distance = 0.42)\n",
    "   - Join 4 and 5 (distance = 0.89)\n",
    "   - Join 2 and 8 (distance = 1.65)\n",
    "   - Join 5-4 and 8 (distance = 1.90)\n",
    "   - Join 1 and 6 (distance = 2.43)\n",
    "\n",
    "4. **Details**:\n",
    "   - **Join 1**: Clusters 9 and 10 at distance 0.42\n",
    "   - **Join 2**: Clusters 4 and 5 at distance 0.89\n",
    "   - **Join 3**: Clusters 2 and 8 at distance 1.65\n",
    "   - **Join 4**: Clusters (4,5) and 8 at distance 1.90\n",
    "   - **Join 5**: Clusters 1 and 6 at distance 2.43\n",
    "\n",
    "#### Part (b)\n",
    "\n",
    "To define four clusters from the given dendrogram:\n",
    "1. **Cut the dendrogram at an appropriate height**.\n",
    "2. **Clusters**:\n",
    "   - Cluster 1: {1, 5, 8}\n",
    "   - Cluster 2: {2, 6, 7, 9}\n",
    "   - Cluster 3: {3, 4}\n",
    "   - Cluster 4: {10}\n",
    "\n",
    "Indicate graphically by drawing a horizontal line at the height that intersects four branches of the dendrogram.\n",
    "\n",
    "### Question 2: Classifiers\n",
    "\n",
    "#### Part (a)\n",
    "\n",
    "For logistic regression, the probability \\( P(\\text{Def} = 1 | \\text{chemx}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\text{chemx})}} \\).\n",
    "\n",
    "Given \\( \\beta_0 = -5.750 \\) and \\( \\beta_1 = 4.419 \\):\n",
    "- Set \\( P(\\text{Def} = 1 | x_0) = 0.5 \\)\n",
    "- \\( 0.5 = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_0)}} \\)\n",
    "- \\( 1 + e^{-(\\beta_0 + \\beta_1 x_0)} = 2 \\)\n",
    "- \\( e^{-(\\beta_0 + \\beta_1 x_0)} = 1 \\)\n",
    "- \\( \\beta_0 + \\beta_1 x_0 = 0 \\)\n",
    "- \\( x_0 = -\\frac{\\beta_0}{\\beta_1} = \\frac{5.750}{4.419} \\approx 1.301 \\) mg/l\n",
    "\n",
    "#### Part (b)\n",
    "\n",
    "To find \\( x_1 \\) such that \\( P(\\text{Def} = 1 | x_1) = 0.4 \\):\n",
    "- \\( 0.4 = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1)}} \\)\n",
    "- \\( e^{-(\\beta_0 + \\beta_1 x_1)} = \\frac{1}{0.4} - 1 = 1.5 \\)\n",
    "- \\( \\beta_0 + \\beta_1 x_1 = \\ln(1.5) \\)\n",
    "- \\( x_1 = \\frac{\\ln(1.5) - \\beta_0}{\\beta_1} = \\frac{\\ln(1.5) + 5.750}{4.419} \\approx 1.823 \\) mg/l\n",
    "\n",
    "#### Part (c)\n",
    "\n",
    "Constructing the classification matrix:\n",
    "- Total with deficiency = 25\n",
    "- Total without deficiency = 35\n",
    "- Incorrect classifications for deficiency = 6\n",
    "- Incorrect classifications for no deficiency = 7\n",
    "\n",
    "|                  | Predicted Deficiency | Predicted No Deficiency |\n",
    "|------------------|----------------------|-------------------------|\n",
    "| Actual Deficiency| 19                   | 6                       |\n",
    "| Actual No Deficiency | 7                | 28                      |\n",
    "\n",
    "- **Sensitivity**: \\( \\frac{19}{25} = 0.76 \\)\n",
    "- **Specificity**: \\( \\frac{28}{35} = 0.80 \\)\n",
    "\n",
    "#### Part (d)\n",
    "\n",
    "Increasing sensitivity to 90%:\n",
    "- Disadvantage: Lower specificity, more false positives.\n",
    "\n",
    "#### Part (e)\n",
    "\n",
    "AUC = 0.78:\n",
    "- Indicates a good model performance.\n",
    "- Criticism: AUC alone doesn't show true positive and false positive rates at specific thresholds.\n",
    "\n",
    "### Question 3: Ridge Regression\n",
    "\n",
    "#### Part (a)\n",
    "\n",
    "Explain Ridge Regression:\n",
    "- **Definition**: Ridge regression adds a penalty to the loss function to shrink coefficients, reducing model complexity.\n",
    "- **Advantages**: Reduces overfitting, handles multicollinearity.\n",
    "- **Disadvantages**: Shrinks all coefficients, doesnâ€™t perform variable selection.\n",
    "\n",
    "#### Part (b)\n",
    "\n",
    "Cross-Validation for Ridge Regression:\n",
    "- Used to select the optimal penalty parameter by minimizing prediction error.\n",
    "\n",
    "#### Part (c)\n",
    "\n",
    "Difference between Ridge and Lasso:\n",
    "- **Ridge**: Shrinks coefficients uniformly.\n",
    "- **Lasso**: Can set some coefficients to zero, performing variable selection.\n",
    "\n",
    "### Question 4: Cross Validation\n",
    "\n",
    "#### Part (a)\n",
    "\n",
    "LOOCV Predicted Values:\n",
    "- Fit model on \\( n-1 \\) data points, predict the left-out point.\n",
    "- Repeat for all points.\n",
    "\n",
    "#### Part (b)\n",
    "\n",
    "Estimate MSE:\n",
    "- Calculate squared errors for each LOOCV prediction.\n",
    "- Average these errors.\n",
    "\n",
    "Given data:\n",
    "\\( \\text{MSE}_{\\text{LOOCV}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_{i, \\text{LOOCV}})^2 \\)\n",
    "\n",
    "#### Part (c)\n",
    "\n",
    "Purpose of LOOCV:\n",
    "- Provides unbiased estimate of model error.\n",
    "\n",
    "#### Part (d)\n",
    "\n",
    "K-Fold vs LOOCV:\n",
    "- **K-Fold**: Data split into K subsets, each subset tested against a model trained on K-1 subsets.\n",
    "- **Advantage**: Computationally efficient.\n",
    "- **Disadvantage**: Less precise compared to LOOCV."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
